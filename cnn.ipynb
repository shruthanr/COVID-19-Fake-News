{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import spacy\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data and creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Constraint_English_Train - Sheet1.csv\")\n",
    "val = pd.read_csv(\"Constraint_English_Val - Sheet1.csv\")\n",
    "test = pd.read_csv(\"english_test_with_labels - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6420\n",
      "Number of validation samples: 2140\n",
      "Number of testing samples: 2140\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(data)}\")\n",
    "print(f\"Number of validation samples: {len(val)}\")\n",
    "print(f\"Number of testing samples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of the training dataset\n",
    "data['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent words in the English language which would not aid classification\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "def process_text(string):\n",
    "    \"\"\"\n",
    "    Replaces \"http\", \"www\" in urls with space.\n",
    "    Replaces ampersand(&) with \"and\"\n",
    "    Removes non-alphanumeric characters\n",
    "    Removes stop words\n",
    "    \"\"\"\n",
    "    text = string.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\",' ',text)    \n",
    "    text = re.sub(r\"www(\\S)+\",' ',text)\n",
    "    text = re.sub(r\"&\",' and ',text)  \n",
    "    tx = text.replace('&amp',' ')\n",
    "    text = re.sub(r\"[^0-9a-zA-Z]+\",' ',text)\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    text = [token.text for token in tok.tokenizer(text)]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x: process_text(x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: process_text(x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for index, row in data.iterrows():\n",
    "    counts.update(row['tweet'])\n",
    "\n",
    "vocab2idx = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2idx[word] = len(words)\n",
    "    words.append(word)\n",
    "\n",
    "idx2vocab = {ii : w for w, ii in vocab2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=32):\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc = np.array([vocab2idx.get(word, vocab2idx[\"UNK\"]) for word in text])\n",
    "    length = min(N, len(enc))\n",
    "    encoded[:length] = enc[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_70091/4113796800.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  data['encoded'] = data['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_70091/4113796800.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  val['encoded'] = val['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_70091/4113796800.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  test['encoded'] = test['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n"
     ]
    }
   ],
   "source": [
    "data['encoded'] = data['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
    "val['encoded'] = val['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
    "test['encoded'] = test['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_encoded'] = data['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "val['label_encoded'] = val['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "test['label_encoded'] = test['label'].apply(lambda x : 0 if x == \"fake\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0]), self.y[idx], self.X[idx][1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "train_data = TweetDataset(data['encoded'], data['label_encoded'])\n",
    "val_data = TweetDataset(val['encoded'], val['label_encoded'])\n",
    "test_data = TweetDataset(test['encoded'], test['label_encoded'])\n",
    "\n",
    "batch_size = 128\n",
    "vocab_size = len(words)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=4, lr=0.001):\n",
    "    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in trainloader:\n",
    "            # print(x)\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            yhat = model(x).squeeze()\n",
    "            # print(y)\n",
    "            # print(yhat)\n",
    "            loss = criterion(yhat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        sum_loss_val = 0.0\n",
    "\n",
    "        for x, y, l in valloader:\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            y_hat = model(x).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "            # print(y_hat)\n",
    "            pred = y_hat >= 0.5\n",
    "            # print(pred)\n",
    "            correct += (pred == y).float().sum()\n",
    "            total += y.shape[0]\n",
    "            sum_loss_val += loss.item()*y.shape[0]\n",
    "\n",
    "        val_acc = correct/total\n",
    "        val_loss = sum_loss_val/total\n",
    "        if i % 2 == 1:\n",
    "            print(\"epoch %d train loss %.3f, val loss %.3f, val accuracy %.3f\" % (i+1, sum_loss/total, val_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "\n",
    "        with open(os.path.join(embedding_name), 'r') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,**kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 1)\n",
    "\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Create multiple one-dimensional convolutional layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(2 * embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Concatenate two embedding layer outputs with shape (batch size, no.\n",
    "        # of tokens, token vector dimension) along vectors\n",
    "        embeddings = torch.cat(\n",
    "            (self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "            \n",
    "        # Per the input format of one-dimensional convolutional layers,\n",
    "        # rearrange the tensor so that the second dimension stores channels\n",
    "\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # For each one-dimensional convolutional layer, after max-over-time\n",
    "        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n",
    "        # obtained. Remove the last dimension and concatenate along channels\n",
    "\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1)\n",
    "\n",
    "        outputs = F.sigmoid(self.decoder(self.dropout(encoding)))\n",
    "        \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(len(vocab2idx), embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) in (nn.Linear, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d.txt')\n",
    "embeds = glove_embedding[idx2vocab]\n",
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 0.492, val loss 0.169, val accuracy 0.936\n",
      "epoch 4 train loss 0.097, val loss 0.174, val accuracy 0.938\n"
     ]
    }
   ],
   "source": [
    "train_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "sum_loss_test = 0.0\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for x, y, l in testloader:\n",
    "    x = x.long()\n",
    "    y = y.float()\n",
    "    y_hat = net(x).squeeze()\n",
    "    loss = criterion(y_hat, y)\n",
    "    pred = y_hat >= 0.5\n",
    "    correct += (pred == y).float().sum()\n",
    "    total += y.shape[0]\n",
    "    sum_loss_test += loss.item()*y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9304)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"cnn_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Set `bidirectional` to True to get a bidirectional RNN\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                               bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "\n",
    "        outs = torch.sigmoid(self.decoder(encoding))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 64, 2\n",
    "net_rnn = BiRNN(len(vocab2idx), embed_size, num_hiddens, num_layers)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "net_rnn.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d.txt')\n",
    "embeds = glove_embedding[idx2vocab]\n",
    "net_rnn.embedding.weight.data.copy_(embeds)\n",
    "# net_rnn.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 0.178, val loss 0.208, val accuracy 0.921\n",
      "epoch 4 train loss 0.017, val loss 0.395, val accuracy 0.919\n",
      "epoch 6 train loss 0.015, val loss 0.496, val accuracy 0.907\n",
      "epoch 8 train loss 0.057, val loss 0.337, val accuracy 0.931\n",
      "epoch 10 train loss 0.011, val loss 0.451, val accuracy 0.916\n"
     ]
    }
   ],
   "source": [
    "train_model(net_rnn,lr=0.01, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9136)\n"
     ]
    }
   ],
   "source": [
    "net_rnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "sum_loss_test = 0.0\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for x, y, l in testloader:\n",
    "    x = x.long()\n",
    "    y = y.float()\n",
    "    y_hat = net_rnn(x).squeeze()\n",
    "    loss = criterion(y_hat, y)\n",
    "    pred = y_hat >= 0.5\n",
    "    correct += (pred == y).float().sum()\n",
    "    total += y.shape[0]\n",
    "    sum_loss_test += loss.item()*y.shape[0]\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Convolution1D\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[cdc, currently, reports, 99031, deaths, gener...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[states, reported, 1121, deaths, small, rise, ...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[21, 22, 23, 6, 13, 24, 25, 26, 27, 21, 22, 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[politically, correct, woman, almost, uses, pa...</td>\n",
       "      <td>fake</td>\n",
       "      <td>[[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[indiafightscorona, 1524, covid, testing, labo...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[populous, states, generate, large, case, coun...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[58, 21, 59, 60, 61, 10, 62, 63, 64, 65, 66, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  [cdc, currently, reports, 99031, deaths, gener...  real   \n",
       "1   2  [states, reported, 1121, deaths, small, rise, ...  real   \n",
       "2   3  [politically, correct, woman, almost, uses, pa...  fake   \n",
       "3   4  [indiafightscorona, 1524, covid, testing, labo...  real   \n",
       "4   5  [populous, states, generate, large, case, coun...  real   \n",
       "\n",
       "                                             encoded  label_encoded  \n",
       "0  [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...              1  \n",
       "1  [[21, 22, 23, 6, 13, 24, 25, 26, 27, 21, 22, 2...              1  \n",
       "2  [[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, ...              0  \n",
       "3  [[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ...              1  \n",
       "4  [[58, 21, 59, 60, 61, 10, 62, 63, 64, 65, 66, ...              1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "          \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "          \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['tweet'])\n",
    "\n",
    "tokenized_data = tokenizer.texts_to_sequences(data['tweet'])\n",
    "X_train = pad_sequences(tokenized_data, maxlen=32, padding='post')\n",
    "y_train = data['label_encoded']\n",
    "\n",
    "tokenized_val = tokenizer.texts_to_sequences(val['tweet'])\n",
    "X_val= pad_sequences(tokenized_val, maxlen=32, padding='post')\n",
    "y_val= val['label_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input = Input(shape=(32,), dtype=\"int32\")\n",
    "embedded_sequences = Embedding(len(word_index)+1, 100, weights=[embedding_matrix])(seq_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Bidirectional(LSTM(32, return_sequences = True), name=\"bi_lstm_0\")(embedded_sequences)\n",
    "# Getting our LSTM outputs\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(32, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "context_vector, attention_weights = Attention(10)(lstm, state_h)\n",
    "x = Dense(128, activation=\"relu\")(context_vector)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x= Dense(128, activation=\"relu\")(x)\n",
    "# dropout = Dropout(0.05)(dense1)\n",
    "output = Dense(1, activation=\"sigmoid\")(x)\n",
    "  \n",
    "model = keras.Model(inputs=seq_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "]\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 12:56:14.945526: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:15.210696: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:15.222959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:15.349113: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:15.358355: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:15.772820: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:15.773323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:16.002259: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 12:56:16.016387: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 17s 73ms/step - loss: 0.4948 - accuracy: 0.7615 - precision: 0.7455 - recall: 0.8265\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 14s 67ms/step - loss: 0.3987 - accuracy: 0.8254 - precision: 0.8235 - recall: 0.8482\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 13s 66ms/step - loss: 0.3603 - accuracy: 0.8436 - precision: 0.8455 - recall: 0.8580\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 13s 66ms/step - loss: 0.3259 - accuracy: 0.8575 - precision: 0.8646 - recall: 0.8628\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.3198 - accuracy: 0.8625 - precision: 0.8670 - recall: 0.8708\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.3122 - accuracy: 0.8653 - precision: 0.8734 - recall: 0.8685\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.3057 - accuracy: 0.8673 - precision: 0.8688 - recall: 0.8792\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.3180 - accuracy: 0.8603 - precision: 0.8677 - recall: 0.8649\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.3188 - accuracy: 0.8650 - precision: 0.8689 - recall: 0.8738\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.3066 - accuracy: 0.8720 - precision: 0.8777 - recall: 0.8777\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.2933 - accuracy: 0.8773 - precision: 0.8871 - recall: 0.8771\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.2989 - accuracy: 0.8723 - precision: 0.8735 - recall: 0.8839\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.2926 - accuracy: 0.8746 - precision: 0.8803 - recall: 0.8801\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 14s 69ms/step - loss: 0.2834 - accuracy: 0.8812 - precision: 0.8866 - recall: 0.8863\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 13s 66ms/step - loss: 0.2765 - accuracy: 0.8826 - precision: 0.8890 - recall: 0.8863\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.2812 - accuracy: 0.8802 - precision: 0.8841 - recall: 0.8875\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 13s 65ms/step - loss: 0.2726 - accuracy: 0.8874 - precision: 0.8942 - recall: 0.8902\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 14s 68ms/step - loss: 0.2632 - accuracy: 0.8939 - precision: 0.9012 - recall: 0.8955\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 13s 66ms/step - loss: 0.2557 - accuracy: 0.8928 - precision: 0.8943 - recall: 0.9018\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 13s 66ms/step - loss: 0.2570 - accuracy: 0.8947 - precision: 0.9025 - recall: 0.8955\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 13:03:52.100000: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 13:03:52.232804: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 13:03:52.240083: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 13:03:52.375474: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-05 13:03:52.382082: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 3s 33ms/step - loss: 0.2906 - accuracy: 0.8794 - precision: 0.8954 - recall: 0.8714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29064199328422546,\n",
       " 0.8794392347335815,\n",
       " 0.8954128623008728,\n",
       " 0.8714285492897034]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd5340ce137fc4c0e52938124d7e2fc4e1139821015397c83e65fe26eb83d7f6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
