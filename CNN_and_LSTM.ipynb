{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import spacy\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data and creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Constraint_English_Train - Sheet1.csv\")\n",
    "val = pd.read_csv(\"Constraint_English_Val - Sheet1.csv\")\n",
    "test = pd.read_csv(\"english_test_with_labels - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6420\n",
      "Number of validation samples: 2140\n",
      "Number of testing samples: 2140\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(data)}\")\n",
    "print(f\"Number of validation samples: {len(val)}\")\n",
    "print(f\"Number of testing samples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of the training dataset\n",
    "data['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent words in the English language which would not aid classification\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "def process_text(string):\n",
    "    \"\"\"\n",
    "    Replaces \"http\", \"www\" in urls with space.\n",
    "    Replaces ampersand(&) with \"and\"\n",
    "    Removes non-alphanumeric characters\n",
    "    Removes stop words\n",
    "    \"\"\"\n",
    "    text = string.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\",' ',text)    \n",
    "    text = re.sub(r\"www(\\S)+\",' ',text)\n",
    "    text = re.sub(r\"&\",' and ',text)  \n",
    "    tx = text.replace('&amp',' ')\n",
    "    text = re.sub(r\"[^0-9a-zA-Z]+\",' ',text)\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    text = [token.text for token in tok.tokenizer(text)]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x: process_text(x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: process_text(x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for index, row in data.iterrows():\n",
    "    counts.update(row['tweet'])\n",
    "\n",
    "vocab2idx = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2idx[word] = len(words)\n",
    "    words.append(word)\n",
    "\n",
    "idx2vocab = {ii : w for w, ii in vocab2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=32):\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc = np.array([vocab2idx.get(word, vocab2idx[\"UNK\"]) for word in text])\n",
    "    length = min(N, len(enc))\n",
    "    encoded[:length] = enc[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_70091/4113796800.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  data['encoded'] = data['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_70091/4113796800.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  val['encoded'] = val['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_70091/4113796800.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  test['encoded'] = test['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n"
     ]
    }
   ],
   "source": [
    "data['encoded'] = data['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
    "val['encoded'] = val['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
    "test['encoded'] = test['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_encoded'] = data['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "val['label_encoded'] = val['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "test['label_encoded'] = test['label'].apply(lambda x : 0 if x == \"fake\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0]), self.y[idx], self.X[idx][1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "train_data = TweetDataset(data['encoded'], data['label_encoded'])\n",
    "val_data = TweetDataset(val['encoded'], val['label_encoded'])\n",
    "test_data = TweetDataset(test['encoded'], test['label_encoded'])\n",
    "\n",
    "batch_size = 128\n",
    "vocab_size = len(words)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=4, lr=0.001):\n",
    "    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in trainloader:\n",
    "            # print(x)\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            yhat = model(x).squeeze()\n",
    "            # print(y)\n",
    "            # print(yhat)\n",
    "            loss = criterion(yhat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        sum_loss_val = 0.0\n",
    "\n",
    "        for x, y, l in valloader:\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            y_hat = model(x).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "            # print(y_hat)\n",
    "            pred = y_hat >= 0.5\n",
    "            # print(pred)\n",
    "            correct += (pred == y).float().sum()\n",
    "            total += y.shape[0]\n",
    "            sum_loss_val += loss.item()*y.shape[0]\n",
    "\n",
    "        val_acc = correct/total\n",
    "        val_loss = sum_loss_val/total\n",
    "        if i % 2 == 1:\n",
    "            print(\"epoch %d train loss %.3f, val loss %.3f, val accuracy %.3f\" % (i+1, sum_loss/total, val_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "\n",
    "        with open(os.path.join(embedding_name), 'r') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,**kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 1)\n",
    "\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Create multiple one-dimensional convolutional layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(2 * embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Concatenate two embedding layer outputs with shape (batch size, no.\n",
    "        # of tokens, token vector dimension) along vectors\n",
    "        embeddings = torch.cat(\n",
    "            (self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "            \n",
    "        # Per the input format of one-dimensional convolutional layers,\n",
    "        # rearrange the tensor so that the second dimension stores channels\n",
    "\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # For each one-dimensional convolutional layer, after max-over-time\n",
    "        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n",
    "        # obtained. Remove the last dimension and concatenate along channels\n",
    "\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1)\n",
    "\n",
    "        outputs = F.sigmoid(self.decoder(self.dropout(encoding)))\n",
    "        \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(len(vocab2idx), embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) in (nn.Linear, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d.txt')\n",
    "embeds = glove_embedding[idx2vocab]\n",
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 0.492, val loss 0.169, val accuracy 0.936\n",
      "epoch 4 train loss 0.097, val loss 0.174, val accuracy 0.938\n"
     ]
    }
   ],
   "source": [
    "train_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "sum_loss_test = 0.0\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for x, y, l in testloader:\n",
    "    x = x.long()\n",
    "    y = y.float()\n",
    "    y_hat = net(x).squeeze()\n",
    "    loss = criterion(y_hat, y)\n",
    "    pred = y_hat >= 0.5\n",
    "    correct += (pred == y).float().sum()\n",
    "    total += y.shape[0]\n",
    "    sum_loss_test += loss.item()*y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9304)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"cnn_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Set `bidirectional` to True to get a bidirectional RNN\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                               bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "\n",
    "        outs = torch.sigmoid(self.decoder(encoding))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 64, 2\n",
    "net_rnn = BiRNN(len(vocab2idx), embed_size, num_hiddens, num_layers)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "net_rnn.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d.txt')\n",
    "embeds = glove_embedding[idx2vocab]\n",
    "net_rnn.embedding.weight.data.copy_(embeds)\n",
    "# net_rnn.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 0.178, val loss 0.208, val accuracy 0.921\n",
      "epoch 4 train loss 0.017, val loss 0.395, val accuracy 0.919\n",
      "epoch 6 train loss 0.015, val loss 0.496, val accuracy 0.907\n",
      "epoch 8 train loss 0.057, val loss 0.337, val accuracy 0.931\n",
      "epoch 10 train loss 0.011, val loss 0.451, val accuracy 0.916\n"
     ]
    }
   ],
   "source": [
    "train_model(net_rnn,lr=0.01, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9136)\n"
     ]
    }
   ],
   "source": [
    "net_rnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "sum_loss_test = 0.0\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for x, y, l in testloader:\n",
    "    x = x.long()\n",
    "    y = y.float()\n",
    "    y_hat = net_rnn(x).squeeze()\n",
    "    loss = criterion(y_hat, y)\n",
    "    pred = y_hat >= 0.5\n",
    "    correct += (pred == y).float().sum()\n",
    "    total += y.shape[0]\n",
    "    sum_loss_test += loss.item()*y.shape[0]\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd5340ce137fc4c0e52938124d7e2fc4e1139821015397c83e65fe26eb83d7f6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
