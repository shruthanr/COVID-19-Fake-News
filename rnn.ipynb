{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa5318f-4ebd-4e51-854a-731011c55c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea1e73df-6c76-4164-b25a-1cbdd201886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "import torch \n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff5491aa-b929-4cbe-bc07-8afd42ed5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Constraint_English_Train - Sheet1.csv\")\n",
    "val = pd.read_csv(\"Constraint_English_Val - Sheet1.csv\")\n",
    "test = pd.read_csv(\"english_test_with_labels - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1bf6f8d-bb7d-4cbd-8969-477ec3cb7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "def process_text(string):\n",
    "    \"\"\"\n",
    "    Replaces \"http\", \"www\" in urls with space.\n",
    "    Replaces ampersand(&) with \"and\"\n",
    "    Removes non-alphanumeric characters\n",
    "    Removes stop words\n",
    "    \"\"\"\n",
    "    text = string.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\",' ',text)    \n",
    "    text = re.sub(r\"www(\\S)+\",' ',text)\n",
    "    text = re.sub(r\"&\",' and ',text)  \n",
    "    tx = text.replace('&amp',' ')\n",
    "    text = re.sub(r\"[^0-9a-zA-Z]+\",' ',text)\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    text = [token.text for token in tok.tokenizer(text)]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51ac5371-5597-4c47-a2c6-8e59e83579b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x: process_text(x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: process_text(x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7877611-f23e-4d8e-8f1a-4445626c811e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[cdc, currently, reports, 99031, deaths, gener...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[states, reported, 1121, deaths, small, rise, ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[politically, correct, woman, almost, uses, pa...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[indiafightscorona, 1524, covid, testing, labo...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[populous, states, generate, large, case, coun...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  [cdc, currently, reports, 99031, deaths, gener...  real\n",
       "1   2  [states, reported, 1121, deaths, small, rise, ...  real\n",
       "2   3  [politically, correct, woman, almost, uses, pa...  fake\n",
       "3   4  [indiafightscorona, 1524, covid, testing, labo...  real\n",
       "4   5  [populous, states, generate, large, case, coun...  real"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81db6200-cfa3-4fb2-94da-b430b02c9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for index, row in data.iterrows():\n",
    "    counts.update(row['tweet'])\n",
    "\n",
    "vocab_to_int = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab_to_int [word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be2ee80f-5f19-4bf0-ad03-74f35df3f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2idx, N=32):\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc = np.array([vocab2idx.get(word, vocab2idx[\"UNK\"]) for word in text])\n",
    "    length = min(N, len(enc))\n",
    "    encoded[:length] = enc[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a2d0ade-52ce-41ea-bbc5-457707260908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[cdc, currently, reports, 99031, deaths, gener...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[states, reported, 1121, deaths, small, rise, ...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[21, 22, 23, 6, 13, 24, 25, 26, 27, 21, 22, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[politically, correct, woman, almost, uses, pa...</td>\n",
       "      <td>fake</td>\n",
       "      <td>[[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[indiafightscorona, 1524, covid, testing, labo...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[populous, states, generate, large, case, coun...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[58, 21, 59, 60, 61, 10, 62, 63, 64, 65, 66, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  [cdc, currently, reports, 99031, deaths, gener...  real   \n",
       "1   2  [states, reported, 1121, deaths, small, rise, ...  real   \n",
       "2   3  [politically, correct, woman, almost, uses, pa...  fake   \n",
       "3   4  [indiafightscorona, 1524, covid, testing, labo...  real   \n",
       "4   5  [populous, states, generate, large, case, coun...  real   \n",
       "\n",
       "                                             encoded  \n",
       "0  [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...  \n",
       "1  [[21, 22, 23, 6, 13, 24, 25, 26, 27, 21, 22, 2...  \n",
       "2  [[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, ...  \n",
       "3  [[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ...  \n",
       "4  [[58, 21, 59, 60, 61, 10, 62, 63, 64, 65, 66, ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['encoded'] = data['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab_to_int), dtype=object))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf66b01f-bdf7-4062-99ee-71881b9b2255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...\n",
       "1       [[21, 22, 23, 6, 13, 24, 25, 26, 27, 21, 22, 2...\n",
       "2       [[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, ...\n",
       "3       [[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ...\n",
       "4       [[58, 21, 59, 60, 61, 10, 62, 63, 64, 65, 66, ...\n",
       "                              ...                        \n",
       "6415    [[5318, 105, 106, 43, 87, 2371, 99, 109, 3004,...\n",
       "6416    [[6606, 8287, 43, 87, 1239, 8288, 5165, 8289, ...\n",
       "6417    [[274, 275, 43, 87, 171, 601, 354, 720, 2538, ...\n",
       "6418    [[2286, 2107, 9428, 11013, 3503, 520, 3084, 80...\n",
       "6419    [[2414, 235, 559, 25, 61, 43, 87, 946, 947, 94...\n",
       "Name: encoded, Length: 6420, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cd0a727-580f-4d34-a054-0b1f8888a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val['encoded'] = val['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab_to_int), dtype=object))\n",
    "test['encoded'] = test['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab_to_int), dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e6c76af-ad3a-4f8b-a8b6-134bfc0df82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  14127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef18a569-f27b-49b3-85b2-5ba5bf189344",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_encoded'] = data['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "val['label_encoded'] = val['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "test['label_encoded'] = test['label'].apply(lambda x : 0 if x == \"fake\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d23e3ad1-691a-4560-9b58-51ba64f03b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0]), self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5ca0121-5649-40ff-8ac2-fd98b4c21749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TweetDataset(data['encoded'], data['label_encoded'])\n",
    "val_data = TweetDataset(val['encoded'], val['label_encoded'])\n",
    "test_data = TweetDataset(test['encoded'], test['label_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1999d45-714f-4c43-9d2d-1268bcee0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "vocab_size = len(words)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "471fdc20-7092-43a9-8e6a-4979253aa097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        \n",
    "        lstm_out = lstm_out[:, -1, :] # getting the last time step output\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         ''' Initializes hidden state '''\n",
    "#         # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "#         # initialized to zero, for hidden state and cell state of LSTM\n",
    "#         weight = next(self.parameters()).data\n",
    "        \n",
    "#         if (train_on_gpu):\n",
    "#             hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "#                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "#         else:\n",
    "#             hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "#                       weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "#         return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "84f95d7b-17de-4deb-b2d9-6055396d84cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(14127, 400, padding_idx=0)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a197f4a7-7651-446b-b74b-ff6561285bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d3930ef-313c-457c-b332-0a799eb20265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "617a1384-4494-427d-af9a-7d23a661a925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/4... Step: 100... Loss: 0.084026... Val Loss: 0.323566\n",
      "Epoch: 4/4... Step: 200... Loss: 0.114681... Val Loss: 0.329537\n"
     ]
    }
   ],
   "source": [
    "epochs = 4 \n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    # h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in trainloader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "       \n",
    "        net.zero_grad()\n",
    "\n",
    "        output, h = net(inputs)\n",
    "\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "     \n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            # val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valloader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                # val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cff77e83-c241-4635-b1c5-6ed5652d9e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32])\n"
     ]
    }
   ],
   "source": [
    "for a, b in trainloader:\n",
    "    print(a.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "afddcc13-210c-4b54-85ee-29eea6b41260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.290\n",
      "Test accuracy: 0.903\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "# h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in testloader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    # h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(testloader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5251b-b712-4b49-b0bf-dcd78d1d2da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF ENV",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
