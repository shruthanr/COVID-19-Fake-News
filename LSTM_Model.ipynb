{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7b442d-ac85-47d6-acee-312678e6b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "import torch \n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0619967-ee68-4b23-884c-dc436fd21681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc4f95a-eded-4c08-b578-35389b5b4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Constraint_English_Train - Sheet1.csv\")\n",
    "val = pd.read_csv(\"Constraint_English_Val - Sheet1.csv\")\n",
    "test = pd.read_csv(\"english_test_with_labels - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42757c8-8a62-4788-b703-de9ac4f5e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6420\n",
      "Number of validation samples: 2140\n",
      "Number of testing samples: 2140\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(data)}\")\n",
    "print(f\"Number of validation samples: {len(val)}\")\n",
    "print(f\"Number of testing samples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ee0793-5ec3-4e94-af4b-ecbf6c526efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of the training dataset\n",
    "data['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f9b86c3-3615-4bbf-abdd-cfa9bfda4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6faa26f1-9b1d-4312-97af-0ca69152ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent words in the English language which would not aid classification\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "def process_text(string):\n",
    "    \"\"\"\n",
    "    Replaces \"http\", \"www\" in urls with space.\n",
    "    Replaces ampersand(&) with \"and\"\n",
    "    Removes non-alphanumeric characters\n",
    "    Removes stop words\n",
    "    \"\"\"\n",
    "    text = string.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\",' ',text)    \n",
    "    text = re.sub(r\"www(\\S)+\",' ',text)\n",
    "    text = re.sub(r\"&\",' and ',text)  \n",
    "    tx = text.replace('&amp',' ')\n",
    "    text = re.sub(r\"[^0-9a-zA-Z]+\",' ',text)\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    text = [token.text for token in tok.tokenizer(text)]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c258ff73-6cdf-4d99-afc0-cca5fbfac502",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x: process_text(x))\n",
    "val['tweet'] = val['tweet'].apply(lambda x: process_text(x))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "837d291f-a461-4e3c-b58f-f1c893fd9ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[cdc, currently, reports, 99031, deaths, gener...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[states, reported, 1121, deaths, small, rise, ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[politically, correct, woman, almost, uses, pa...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[indiafightscorona, 1524, covid, testing, labo...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[populous, states, generate, large, case, coun...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  [cdc, currently, reports, 99031, deaths, gener...  real\n",
       "1   2  [states, reported, 1121, deaths, small, rise, ...  real\n",
       "2   3  [politically, correct, woman, almost, uses, pa...  fake\n",
       "3   4  [indiafightscorona, 1524, covid, testing, labo...  real\n",
       "4   5  [populous, states, generate, large, case, coun...  real"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7699ab95-ab7a-468a-a7e9-6a0db4234e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.5398753894081"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data['tweet'].apply(lambda x : len(x)))/len(data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5915bcb-fb7d-4a05-974f-8dbfb3aade54",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for index, row in data.iterrows():\n",
    "    counts.update(row['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee40fb75-75f2-4ee8-adc9-954d84f20f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2idx[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fde5104b-ecc2-467d-995c-028f130267ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=32):\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc = np.array([vocab2idx.get(word, vocab2idx[\"UNK\"]) for word in text])\n",
    "    length = min(N, len(enc))\n",
    "    encoded[:length] = enc[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d48c30f7-e61d-479c-a442-ab8054227d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_92559/2487720325.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data['encoded'] = data['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[cdc, currently, reports, 99031, deaths, gener...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[states, reported, 1121, deaths, small, rise, ...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[21, 22, 23, 6, 13, 24, 25, 26, 27, 21, 22, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[politically, correct, woman, almost, uses, pa...</td>\n",
       "      <td>fake</td>\n",
       "      <td>[[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[indiafightscorona, 1524, covid, testing, labo...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[populous, states, generate, large, case, coun...</td>\n",
       "      <td>real</td>\n",
       "      <td>[[58, 21, 59, 60, 61, 10, 62, 63, 64, 65, 66, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  [cdc, currently, reports, 99031, deaths, gener...  real   \n",
       "1   2  [states, reported, 1121, deaths, small, rise, ...  real   \n",
       "2   3  [politically, correct, woman, almost, uses, pa...  fake   \n",
       "3   4  [indiafightscorona, 1524, covid, testing, labo...  real   \n",
       "4   5  [populous, states, generate, large, case, coun...  real   \n",
       "\n",
       "                                             encoded  \n",
       "0  [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...  \n",
       "1  [[21, 22, 23, 6, 13, 24, 25, 26, 27, 21, 22, 2...  \n",
       "2  [[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, ...  \n",
       "3  [[41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ...  \n",
       "4  [[58, 21, 59, 60, 61, 10, 62, 63, 64, 65, 66, ...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['encoded'] = data['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9be3f0f4-ca4c-40e6-aa56-7c3b0fc7b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_92559/17374283.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val['encoded'] = val['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
      "/var/folders/0k/h8p0bgts4g5113f9brnt4l4w0000gn/T/ipykernel_92559/17374283.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test['encoded'] = test['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n"
     ]
    }
   ],
   "source": [
    "val['encoded'] = val['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))\n",
    "test['encoded'] = test['tweet'].apply(lambda x: np.array(encode_sentence(x,vocab2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a1c683b-5796-459e-9f32-6e770db44bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_encoded'] = data['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "val['label_encoded'] = val['label'].apply(lambda x : 0 if x == \"fake\" else 1)\n",
    "test['label_encoded'] = test['label'].apply(lambda x : 0 if x == \"fake\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "545b1058-1f54-448e-a409-ff16911fba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f158fe7-bf09-4367-b73f-cc1a29e8e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0]), self.y[idx], self.X[idx][1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "534ea0b6-00b7-4d02-91ab-93f286c13ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TweetDataset(data['encoded'], data['label_encoded'])\n",
    "val_data = TweetDataset(val['encoded'], val['label_encoded'])\n",
    "test_data = TweetDataset(test['encoded'], test['label_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96eb722a-2b9e-4deb-8b41-5ca5667d3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "vocab_size = len(words)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd03639c-cc6c-44d3-b28b-0d253a8db15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in trainloader:\n",
    "            print(x)\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            yhat = model(x, l).squeeze()\n",
    "            # print(y)\n",
    "            # print(yhat)\n",
    "            loss = criterion(yhat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        sum_loss_val = 0.0\n",
    "\n",
    "        for x, y, l in valloader:\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            y_hat = model(x, l).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "            # print(y_hat)\n",
    "            pred = y_hat >= 0.5\n",
    "            # print(pred)\n",
    "            correct += (pred == y).float().sum()\n",
    "            total += y.shape[0]\n",
    "            sum_loss_val += loss.item()*y.shape[0]\n",
    "\n",
    "        val_acc = correct/total\n",
    "        val_loss = sum_loss/total\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2b7812f-6586-4505-b898-083f89f65a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super(LSTM_fixed, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return F.sigmoid(self.linear(ht[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b5d405a-41f9-4749-8cac-2bd048f71750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fixed =  LSTM_fixed(vocab_size, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c3ba40c8-737d-4465-9b6c-132598e530a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.583, val loss 1.583, val accuracy 0.767\n",
      "train loss 0.244, val loss 0.244, val accuracy 0.902\n",
      "train loss 0.069, val loss 0.069, val accuracy 0.899\n",
      "train loss 0.074, val loss 0.074, val accuracy 0.893\n",
      "train loss 0.018, val loss 0.018, val accuracy 0.900\n",
      "train loss 0.032, val loss 0.032, val accuracy 0.900\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e2adb274-2334-45ac-adf9-c718f4b213db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fixed.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "sum_loss_test = 0.0\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for x, y, l in testloader:\n",
    "    x = x.long()\n",
    "    y = y.float()\n",
    "    y_hat = model_fixed(x, l).squeeze()\n",
    "    loss = criterion(y_hat, y)\n",
    "    pred = y_hat >= 0.5\n",
    "    correct += (pred == y).float().sum()\n",
    "    total += y.shape[0]\n",
    "    sum_loss_test += loss.item()*y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "09c3adca-efb2-413c-b18b-389d7db43e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9079)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "58ff206d-05cf-42da-8a17-45da84033db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\"./glove.6B.100d.txt\"):\n",
    "    word_vectors = {}\n",
    "    with open(glove_file) as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3d4e412-f49c-40ac-b386-d388384e17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_emb_matrix(pretrained, word_counts, emb_size = 100):\n",
    "    \"\"\" \n",
    "    Creates embedding matrix from word vectors\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3e48c2ad-33f7-49fe-a11d-cfc392d52779",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = glove_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b11170a7-7f61-4da0-9466-0d7d39fc7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        # self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return F.sigmoid(self.linear(ht[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2efa2ad7-de0f-4811-9a9d-1b15a3fe079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_glove_vecs(vocab_size, 100, 64, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5e6fc2dc-401f-4b57-bcd8-3377633aeee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.004, val loss 1.004, val accuracy 0.886\n",
      "train loss 0.320, val loss 0.320, val accuracy 0.930\n",
      "train loss 0.017, val loss 0.017, val accuracy 0.929\n",
      "train loss 0.058, val loss 0.058, val accuracy 0.921\n",
      "train loss 0.005, val loss 0.005, val accuracy 0.924\n",
      "train loss 0.030, val loss 0.030, val accuracy 0.924\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4c169c2e-3998-4d39-8b5b-b876ea70cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fixed.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "sum_loss_test = 0.0\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for x, y, l in testloader:\n",
    "    x = x.long()\n",
    "    y = y.float()\n",
    "    y_hat = model(x, l).squeeze()\n",
    "    loss = criterion(y_hat, y)\n",
    "    pred = y_hat >= 0.5\n",
    "    correct += (pred == y).float().sum()\n",
    "    total += y.shape[0]\n",
    "    sum_loss_test += loss.item()*y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "94b7ef57-9edf-470d-9783-7857f65d7bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9243)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e918d-f05e-4015-8607-c531933232e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd5340ce137fc4c0e52938124d7e2fc4e1139821015397c83e65fe26eb83d7f6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
